{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Install Dependencies**"
      ],
      "metadata": {
        "id": "oeBgV5cobrTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio pymupdf sentence-transformers huggingface_hub python-dotenv numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23-aTXOxbuwb",
        "outputId": "2fecde31-8411-4e1c-adab-23e710244eee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.39.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Extraction Code**\n"
      ],
      "metadata": {
        "id": "Sv1ncodZRbqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "KEY_SECTIONS = [\n",
        "    \"exclusion\", \"waiting period\", \"specified disease\", \"specified procedure\",\n",
        "    \"pre-existing\", \"joint replacement\", \"coverage\", \"benefit\", \"day care\"\n",
        "]\n",
        "SECTION_REGEX = re.compile(\"|\".join(rf\"({k})\" for k in KEY_SECTIONS), re.IGNORECASE)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = [page.get_text() for page in doc]\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def smart_chunk_policy(text, max_words=500, stride=250):\n",
        "    lines = text.splitlines()\n",
        "    chunks = []\n",
        "    words = []\n",
        "    headers = []\n",
        "    for i, line in enumerate(lines):\n",
        "        # Detect key section starts, save current buffer as its own chunk\n",
        "        if SECTION_REGEX.search(line):\n",
        "            if words:\n",
        "                chunks.append(\" \".join(words))\n",
        "                words = []\n",
        "            headers.append(line.strip())\n",
        "            continue\n",
        "        # Normal chunking\n",
        "        for word in line.split():\n",
        "            words.append(word)\n",
        "            if len(words) >= max_words:\n",
        "                chunk = \"\"\n",
        "                if headers:\n",
        "                    chunk += \" \".join(headers) + \"\\n\"\n",
        "                chunk += \" \".join(words)\n",
        "                chunks.append(chunk)\n",
        "                # Overlap/stride\n",
        "                words = words[-stride:] if stride else []\n",
        "    # Flush last\n",
        "    if words:\n",
        "        chunk = \"\"\n",
        "        if headers:\n",
        "            chunk += \" \".join(headers) + \"\\n\"\n",
        "        chunk += \" \".join(words)\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Usage\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = smart_chunk_policy(text, max_words=500, stride=250)\n",
        "with open(\"chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in chunks:\n",
        "        f.write(c.replace('\\n', ' ') + '\\n')\n",
        "print(f\"Produced {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "rNLcRAoc47n_",
        "outputId": "3a2534de-86a8-4139-d4f9-057a2539bc62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc35db60-63e9-4dba-90c1-3bba083e7d3a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc35db60-63e9-4dba-90c1-3bba083e7d3a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Doc 1.pdf to Doc 1.pdf\n",
            "Produced 184 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Evaluation**"
      ],
      "metadata": {
        "id": "5vbAUTC-Zv3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **a. Structural**"
      ],
      "metadata": {
        "id": "NbfH1Nk5RUpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# 1) Read your pre‐chunked document\n",
        "with open(\"chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_chunks = f.read()\n",
        "\n",
        "# 2) Define the user query\n",
        "user_query = \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\"\n",
        "\n",
        "# 3) Build the chat messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an expert policy-decision assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User Query:\n",
        "\"{user_query}\"\n",
        "\n",
        "Document Chunks:\n",
        "{document_chunks}\n",
        "\n",
        "Instructions:\n",
        "- Extract the fields: age, procedure, location, and policy duration.\n",
        "- Identify the relevant clause(s) in the document chunks that govern knee surgery coverage.\n",
        "- Decide whether the claim is APPROVED or REJECTED.\n",
        "- Return STRICT JSON with keys:\n",
        "    • decision: \"approved\" or \"rejected\"\n",
        "    • amount: numeric (or null)\n",
        "    • justification: a list of objects, each with:\n",
        "        {{\n",
        "          \"clause\": <exact clause text>,\n",
        "          \"reason\": <how it applies>\n",
        "        }}\n",
        "\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 4) Initialize the HF Inference client\n",
        "# Make sure HF_TOKEN is set in your Colab env (via !export or colab secrets)\n",
        "client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "# 5) Call chat_completion\n",
        "response = client.chat_completion(\n",
        "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
        "    messages=messages,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "# 6) Extract and print the assistant’s reply\n",
        "# The content is in response.choices[0].message.content\n",
        "decision_json = response.choices[0].message.content\n",
        "print(decision_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI6HfXMr6fBk",
        "outputId": "28b7111a-d3cc-4ac7-8237-6df89eccc1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"decision\": \"approved\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": [\n",
            "    {\n",
            "      \"clause\": \"SECTION C) BENEFITS COVERED UNDER THE POLICY exclusions contained or otherwise expressed in this Policy. PART A- COVERAGE- Domestic (Within India Only, for Imperial and Imperial Plus Plans) I. IN-PATIENT BENEFITS FOR DOMESTIC COVER ii. We have accepted Your Claim under \\\"In-patient Hospitalization Treatment\\\" or \\\"Day Care Procedures\\\" section of Subject otherwise to the terms, conditions and exclusions of the Policy. 5. Day Care Procedures for Day care procedures / Surgeries taken as an Inpatient in a Hospital or Day Care Centre but not in the outpatient department.\",\n",
            "      \"reason\": \"The policy covers inpatient hospitalizations and day care procedures, which includes knee surgery.\"\n",
            "    },\n",
            "    {\n",
            "      \"clause\": \"The above coverage is subject to fulfilment of following conditions: Exclusions: Mental Illness Treatment does not cover: d. For autism spectrum disorder, admissions, stays or day care treatment at specialised educational facilities are Annexure III subject to the Sum Insured, terms, conditions and definitions, exclusions contained or otherwise TABLE OF BENEFITS FOR DOMESTIC COVER Day Care Procedures PART B- COVERAGE- International I. IN-PATIENT BENEFITS FOR INTERNATIONAL COVER This cover is subject to the Sum Insured, sub-limits, Deductibles, terms, conditions and definitions, exclusions This cover is subject to the Sum Insured, sub-limits, terms, conditions and definitions, exclusions contained or This cover is subject to the Sum Insured, sub-limits, terms, conditions and definitions, exclusions contained or We have accepted Your Claim under \\\"In-patient Hospitalization Treatment\\\" or \\\"Day Care Procedures\\\" This cover is subject to the Sum Insured, sub-limits, terms, conditions and definitions, exclusions contained or 5. Day Care Procedures for Day Care Procedures / Surgeries taken as an Inpatient in a Hospital or Day Care Centre but not in the outpatient department. List of Day Care Procedures is as given in the annexure I of Policy wordings.\",\n",
            "      \"reason\": \"The policy does not list knee surgery as an excluded procedure, implying it is covered.\"\n",
            "    },\n",
            "    {\n",
            "      \"clause\": \"The Policy shall be void and all premium paid thereon shall be forfeited to the Company, in the event of misrepresentation, mis-description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b. Practical**"
      ],
      "metadata": {
        "id": "iVwfRWguRhVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1) Read your pre‐chunked document from chunks.txt\n",
        "# -----------------------------------------------------------------------------\n",
        "with open(\"chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_chunks = f.read()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2) Define the user query\n",
        "# -----------------------------------------------------------------------------\n",
        "user_query = \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) Build the chat messages, enforcing decision = “approved” or “rejected”\n",
        "# -----------------------------------------------------------------------------\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an expert policy-decision assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User Query:\n",
        "\"{user_query}\"\n",
        "\n",
        "Document Chunks:\n",
        "{document_chunks}\n",
        "\n",
        "Instructions:\n",
        "1. Extract the fields: age, procedure, location, and policy duration.\n",
        "2. Identify the relevant clause(s) in the document chunks that govern knee surgery coverage.\n",
        "3. Decide whether the claim is \"approved\" or \"rejected\" (exactly those strings).\n",
        "4. Return STRICT JSON with keys:\n",
        "   • decision: \"approved\" or \"rejected\"\n",
        "   • amount: numeric (or null)\n",
        "   • justification: a list of objects, each with:\n",
        "       {{\n",
        "         \"clause\": <exact clause text>,\n",
        "         \"reason\": <how it applies>\n",
        "       }}\n",
        "\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4) Initialize the HF Inference client\n",
        "# -----------------------------------------------------------------------------\n",
        "# Ensure your HF token is set:\n",
        "#   export HF_TOKEN=\"your_token_here\"\n",
        "client = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5) Call the conversational endpoint\n",
        "# -----------------------------------------------------------------------------\n",
        "response = client.chat_completion(\n",
        "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
        "    messages=messages,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6) Extract and print the model’s JSON reply\n",
        "# -----------------------------------------------------------------------------\n",
        "reply = response.choices[0].message.content\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGBXEnZ86frC",
        "outputId": "3d4cc75e-b56a-47f5-cbde-eefa543f99a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"decision\": \"approved\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": [\n",
            "    {\n",
            "      \"clause\": \"In-patient Hospitalization Treatment Limits INR 3,750,000 INR 5,600,000 INR 7,500,000 INR 11,200,000 INR 18,750,000 INR 37,500,000\",\n",
            "      \"reason\": \"The policy covers knee surgery under the In-patient Hospitalization Treatment limits, which are sufficiently high to cover the procedure.\"\n",
            "    },\n",
            "    {\n",
            "      \"clause\": \"Pre-hospitalisation 60 days Post-hospitalisation 180 days\",\n",
            "      \"reason\": \"The policy allows for pre-hospitalization and post-hospitalization expenses, which are relevant for knee surgery.\"\n",
            "    },\n",
            "    {\n",
            "      \"clause\": \"The Policy shall be void and all premium paid thereon shall be forfeited to the Company, in the event of misrepresentation, mis-description or non-disclosure of any material fact.\",\n",
            "      \"reason\": \"No misrepresentation or non-disclosure is indicated in the query, so the claim is not void.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzHIHW-MC7BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}